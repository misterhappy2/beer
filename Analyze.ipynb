{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYZE\n",
    "the data\n",
    "\n",
    "This is a work in progress  \n",
    "\n",
    "Vectorize the data in beer.review  \n",
    "Diminish the importance of common words  \n",
    "Compare ML algorithms to use the review data to predict beer.style  \n",
    "\n",
    "Compare ML algorithms to predict beer.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 80818\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>brewery</th>\n",
       "      <th>style</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Rock Ale</td>\n",
       "      <td>Big Rock Brewery</td>\n",
       "      <td>Scottish Ale</td>\n",
       "      <td>3.90</td>\n",
       "      <td>smell  soft hop aroma with significant malt scents. this one smells very creamy. taste  and creamy it is. the traditional irish flavors come out at the tongue. this is creamy, not like a cream ale, but close. the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flip Ale</td>\n",
       "      <td>Dogfish Head Craft Brewery</td>\n",
       "      <td>Old Ale</td>\n",
       "      <td>4.08</td>\n",
       "      <td>on tap at dfh rehoboth... collab with eatily... cardamom and red wine must. golden orange. .no head. typical dfh yeast aroma. ..some spice and maybe a belgian influence. sweet spicy and somewhat fruity.. not much ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Almond Marzen Project - Beer Camp #26</td>\n",
       "      <td>Sierra Nevada Brewing Co.</td>\n",
       "      <td>Märzen / Oktoberfest</td>\n",
       "      <td>3.78</td>\n",
       "      <td>nice auburn impressions, tons of clarity, solid inch of off white head.   aroma was a little bit sweet and nutty. taste gave a little more sweetness, stayed away from hops and bitterness, relatively light bodied.  no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        name                     brewery  \\\n",
       "0                               Big Rock Ale            Big Rock Brewery   \n",
       "1                                   Flip Ale  Dogfish Head Craft Brewery   \n",
       "2  The Almond Marzen Project - Beer Camp #26   Sierra Nevada Brewing Co.   \n",
       "\n",
       "                  style  rating  \\\n",
       "0          Scottish Ale    3.90   \n",
       "1               Old Ale    4.08   \n",
       "2  Märzen / Oktoberfest    3.78   \n",
       "\n",
       "                                                                                                                                                                                                                        review  \n",
       "0  smell  soft hop aroma with significant malt scents. this one smells very creamy. taste  and creamy it is. the traditional irish flavors come out at the tongue. this is creamy, not like a cream ale, but close. the m...  \n",
       "1   on tap at dfh rehoboth... collab with eatily... cardamom and red wine must. golden orange. .no head. typical dfh yeast aroma. ..some spice and maybe a belgian influence. sweet spicy and somewhat fruity.. not much ol...  \n",
       "2  nice auburn impressions, tons of clarity, solid inch of off white head.   aroma was a little bit sweet and nutty. taste gave a little more sweetness, stayed away from hops and bitterness, relatively light bodied.  no...  "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT MODULES AND THE DATA SET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('beer.csv', header=0)\n",
    "df_copy = df  #save a copy of dataframe for reference. \n",
    "print('length',len(df))\n",
    "pd.set_option('max_colwidth', 220)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df original length 80818\n",
      "length without short reviews 49141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brentmarijensen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>brewery</th>\n",
       "      <th>style</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Rock Ale</td>\n",
       "      <td>Big Rock Brewery</td>\n",
       "      <td>Scottish Ale</td>\n",
       "      <td>3.90</td>\n",
       "      <td>smell  soft hop aroma with significant malt scents. this one smells very creamy. taste  and creamy it is. the traditional irish flavors come out at the tongue. this is creamy, not like a cream ale, but close. the m...</td>\n",
       "      <td>smell soft hop aroma significant malt scents one smells creamy taste creamy traditional irish flavors come tongue creamy like cream ale close malt big buttery smooth hops unique sharp hop flavor easy saturated well m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flip Ale</td>\n",
       "      <td>Dogfish Head Craft Brewery</td>\n",
       "      <td>Old Ale</td>\n",
       "      <td>4.08</td>\n",
       "      <td>on tap at dfh rehoboth... collab with eatily... cardamom and red wine must. golden orange. .no head. typical dfh yeast aroma. ..some spice and maybe a belgian influence. sweet spicy and somewhat fruity.. not much ol...</td>\n",
       "      <td>tap dfh rehoboth collab eatily cardamom red wine must golden orange head typical dfh yeast aroma spice maybe belgian influence sweet spicy somewhat fruity much old ale characteristic light still tasty cardamom add ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Almond Marzen Project - Beer Camp #26</td>\n",
       "      <td>Sierra Nevada Brewing Co.</td>\n",
       "      <td>Märzen / Oktoberfest</td>\n",
       "      <td>3.78</td>\n",
       "      <td>nice auburn impressions, tons of clarity, solid inch of off white head.   aroma was a little bit sweet and nutty. taste gave a little more sweetness, stayed away from hops and bitterness, relatively light bodied.  no...</td>\n",
       "      <td>nice auburn impressions tons clarity solid inch white head aroma little bit sweet nutty taste gave little sweetness stayed away hops bitterness relatively light bodied nothing almond came obvious kind fancied oktober...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name                     brewery  \\\n",
       "index                                                                          \n",
       "0                                   Big Rock Ale            Big Rock Brewery   \n",
       "1                                       Flip Ale  Dogfish Head Craft Brewery   \n",
       "2      The Almond Marzen Project - Beer Camp #26   Sierra Nevada Brewing Co.   \n",
       "\n",
       "                      style  rating  \\\n",
       "index                                 \n",
       "0              Scottish Ale    3.90   \n",
       "1                   Old Ale    4.08   \n",
       "2      Märzen / Oktoberfest    3.78   \n",
       "\n",
       "                                                                                                                                                                                                                            review  \\\n",
       "index                                                                                                                                                                                                                                \n",
       "0      smell  soft hop aroma with significant malt scents. this one smells very creamy. taste  and creamy it is. the traditional irish flavors come out at the tongue. this is creamy, not like a cream ale, but close. the m...   \n",
       "1       on tap at dfh rehoboth... collab with eatily... cardamom and red wine must. golden orange. .no head. typical dfh yeast aroma. ..some spice and maybe a belgian influence. sweet spicy and somewhat fruity.. not much ol...   \n",
       "2      nice auburn impressions, tons of clarity, solid inch of off white head.   aroma was a little bit sweet and nutty. taste gave a little more sweetness, stayed away from hops and bitterness, relatively light bodied.  no...   \n",
       "\n",
       "                                                                                                                                                                                                                      clean_review  \n",
       "index                                                                                                                                                                                                                               \n",
       "0      smell soft hop aroma significant malt scents one smells creamy taste creamy traditional irish flavors come tongue creamy like cream ale close malt big buttery smooth hops unique sharp hop flavor easy saturated well m...  \n",
       "1      tap dfh rehoboth collab eatily cardamom red wine must golden orange head typical dfh yeast aroma spice maybe belgian influence sweet spicy somewhat fruity much old ale characteristic light still tasty cardamom add ni...  \n",
       "2      nice auburn impressions tons clarity solid inch white head aroma little bit sweet nutty taste gave little sweetness stayed away hops bitterness relatively light bodied nothing almond came obvious kind fancied oktober...  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA PREP\n",
    "print('df original length',len(df))\n",
    "# drop all reviews with < 20 characters\n",
    "df = df[df['review'].map(len) > 20]\n",
    "print('length without short reviews',len(df))\n",
    "\n",
    "# reset dataframe index for the shortened dataframe\n",
    "df['index'] = np.arange(len(df))\n",
    "df = df.set_index('index')\n",
    "\n",
    "# Change review to a string of words.  remove non-letters, make lower case, split into words.  \n",
    "# Remove stopwords (common words.)  Join back together into a long string of words. \n",
    "def review_to_words(review):\n",
    "    letters_only = re.sub('[^a-zA-Z]',' ', review)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words('english'))  \n",
    "    good_words = [w for w in words if not w in stops]\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in good_words]\n",
    "    return(' '.join(good_words))\n",
    "\n",
    "# clean the reviews\n",
    "df['clean_review'] = df['review'].apply(review_to_words)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49141"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.name.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ADDITIONAL FEATURE ENGINEERING\n",
    "# review length\n",
    "df['review_length'] = df['review'].apply(len)\n",
    "\n",
    "# average word length\n",
    "def avg_word_len(words):\n",
    "    separate_words = words.split()\n",
    "    count_words = (len(separate_words))    # number of words\n",
    "    if count_words> 0:\n",
    "        characters = len(words)  # length of text\n",
    "        avg = (characters - count_words+1)/count_words\n",
    "    else:\n",
    "        avg = 5.65  # this is the mean of 49000 reviews    \n",
    "    return avg   \n",
    "\n",
    "df['avg_word_length'] = df['clean_review'].apply(avg_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 1928.5620764738203\n",
      "max:  22085\n",
      "min:  21\n",
      "\n",
      "mean: 5.684227299077345\n",
      "max:  11.0\n",
      "min:  3.0\n"
     ]
    }
   ],
   "source": [
    "print('mean:',df['review_length'].mean())\n",
    "print('max: ',df['review_length'].max())\n",
    "print('min: ',df['review_length'].min())\n",
    "print('')\n",
    "print('mean:',df['avg_word_length'].mean())\n",
    "print('max: ',df['avg_word_length'].max())\n",
    "print('min: ',df['avg_word_length'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of styles used: 104\n",
      "\n",
      "Index(['Altbier', 'American Adjunct Lager', 'American Amber / Red Ale',\n",
      "       'American Amber / Red Lager', 'American Barleywine',\n",
      "       'American Black Ale', 'American Blonde Ale', 'American Brown Ale',\n",
      "       'American Dark Wheat Ale', 'American Double / Imperial IPA',\n",
      "       ...\n",
      "       'Scotch Ale / Wee Heavy', 'Scottish Ale',\n",
      "       'Scottish Gruit / Ancient Herbed Ale', 'Smoked Beer', 'Tripel',\n",
      "       'Vienna Lager', 'Weizenbock', 'Wheatwine', 'Winter Warmer', 'Witbier'],\n",
      "      dtype='object', name='style', length=104)\n"
     ]
    }
   ],
   "source": [
    "styles = df.groupby(['style']).size() \n",
    "print('Number of styles used:', len(styles))\n",
    "print('')\n",
    "print(styles.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length without uncommon styles 46861\n"
     ]
    }
   ],
   "source": [
    "# COMBINE SIMILAR STYLES OF BEER.  \n",
    "\n",
    "df['style'].replace('Saison / Farmhouse Ale', 'Farm Ale', inplace=True)\n",
    "df['style'].replace('Bière de Garde', 'Farm Ale', inplace=True)\n",
    "df['style'].replace('American IPA', 'IPA', inplace=True)\n",
    "df['style'].replace('English India Pale Ale (IPA)', 'IPA', inplace=True)\n",
    "df['style'].replace('Belgian IPA', 'IPA', inplace=True)\n",
    "df['style'].replace('Scotch Ale / Wee Heavy', 'Scottish Ale', inplace=True)\n",
    "df['style'].replace('American Pale Ale (APA)', 'Pale Ale', inplace=True)\n",
    "df['style'].replace('English Pale Ale', 'Pale Ale', inplace=True)\n",
    "df['style'].replace('Belgian Pale Ale', 'Pale Ale', inplace=True)\n",
    "df['style'].replace('American Brown Ale', 'Brown Ale', inplace=True)\n",
    "df['style'].replace('English Brown Ale', 'Brown Ale', inplace=True)\n",
    "df['style'].replace('English Dark Mild Ale', 'Brown Ale', inplace=True)\n",
    "df['style'].replace('American Stout', 'Stout', inplace=True)\n",
    "df['style'].replace('English Stout', 'Stout', inplace=True)\n",
    "df['style'].replace('Milk / Sweet Stout', 'Stout', inplace=True)\n",
    "df['style'].replace('Oatmeal Stout', 'Stout', inplace=True)\n",
    "df['style'].replace('Oatmeal Stout', 'Stout', inplace=True)\n",
    "df['style'].replace('American Double / Imperial Stout', 'Imperial Stout', inplace=True)\n",
    "df['style'].replace('Russian Imperial', 'Imperial Stout', inplace=True)\n",
    "df['style'].replace('American Porter', 'Porter', inplace=True)\n",
    "df['style'].replace('Baltic Porter', 'Porter', inplace=True)\n",
    "df['style'].replace('English Porter', 'Porter', inplace=True)\n",
    "df['style'].replace('American Amber / Red Lager', 'Lager', inplace=True)\n",
    "df['style'].replace('Vienna Lager', 'Lager', inplace=True)\n",
    "df['style'].replace('German Pilsener', 'Lager', inplace=True)\n",
    "df['style'].replace('Munich Helles Lager', 'Lager', inplace=True)\n",
    "df['style'].replace('American Adjunct Lager', 'American Lager', inplace=True)\n",
    "df['style'].replace('American Pale Lager', 'American Lager', inplace=True)\n",
    "df['style'].replace('American Barleywine', 'Barleywine', inplace=True)\n",
    "df['style'].replace('English Barleywine', 'Barleywine', inplace=True)\n",
    "df['style'].replace('English Bitter', 'Bitter', inplace=True)\n",
    "df['style'].replace('Extra Special / Strong Bitter (ESB)', 'Bitter', inplace=True)\n",
    "df['style'].replace('American Pale Wheat Ale', 'Wheat', inplace=True)\n",
    "df['style'].replace('Witbier', 'Wheat', inplace=True)\n",
    "#df['style'].replace('Witbier', 'Wheat', inplace=True)\n",
    "\n",
    "\n",
    "# remove uncommon styles (in EDA, I found 13 styles with fewer than 60 reviews)\n",
    "#uncommon = [ 'American Dark Wheat Ale','Bière de Champagne / Bière Brut', 'Black & Tan', \n",
    "#            'Eisbock', 'Faro', 'Gueuze', 'Happoshu', 'Japanese Rice Lager', 'Kristalweizen',\n",
    "#            'Kvass', 'Lambic - Unblended','Roggenbier', 'Sahti' ]   \n",
    "labels = df.groupby(['style']).size() \n",
    "uncommon = labels[labels<160]\n",
    "df = df.loc[~df['style'].isin(uncommon.index)]\n",
    "print('length without uncommon styles',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 styles excluded:\n",
      "Index(['American Dark Wheat Ale', 'American Double / Imperial Pilsner',\n",
      "       'American Malt Liquor', 'Bière de Champagne / Bière Brut',\n",
      "       'Black & Tan', 'Braggot', 'California Common / Steam Beer',\n",
      "       'Chile Beer', 'Dortmunder / Export Lager', 'Eisbock',\n",
      "       'English Pale Mild Ale', 'English Strong Ale', 'Euro Strong Lager',\n",
      "       'Faro', 'Flanders Oud Bruin', 'Flanders Red Ale',\n",
      "       'Foreign / Export Stout', 'Gueuze', 'Happoshu', 'Japanese Rice Lager',\n",
      "       'Kristalweizen', 'Kvass', 'Lambic - Unblended', 'Low Alcohol Beer',\n",
      "       'Rauchbier', 'Roggenbier', 'Sahti',\n",
      "       'Scottish Gruit / Ancient Herbed Ale', 'Weizenbock', 'Wheatwine'],\n",
      "      dtype='object', name='style')\n"
     ]
    }
   ],
   "source": [
    "print(len(uncommon), 'styles excluded:')\n",
    "print(uncommon.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6268"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.IPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style\n",
      "IPA                               6268\n",
      "Pale Ale                          4164\n",
      "American Double / Imperial IPA    2677\n",
      "Farm Ale                          2387\n",
      "Stout                             2305\n",
      "Porter                            2144\n",
      "American Wild Ale                 1769\n",
      "Imperial Stout                    1591\n",
      "Brown Ale                         1494\n",
      "American Amber / Red Ale          1424\n",
      "Lager                             1408\n",
      "Wheat                             1310\n",
      "Bitter                            1171\n",
      "Fruit / Vegetable Beer            1010\n",
      "American Lager                     822\n",
      "Barleywine                         799\n",
      "American Blonde Ale                770\n",
      "Hefeweizen                         755\n",
      "Russian Imperial Stout             728\n",
      "Scottish Ale                       671\n",
      "Euro Pale Lager                    566\n",
      "American Black Ale                 556\n",
      "Berliner Weissbier                 548\n",
      "Märzen / Oktoberfest               535\n",
      "Tripel                             516\n",
      "Belgian Strong Pale Ale            474\n",
      "American Strong Ale                438\n",
      "Czech Pilsener                     429\n",
      "Herbed / Spiced Beer               415\n",
      "Belgian Strong Dark Ale            407\n",
      "Kölsch                             391\n",
      "Rye Beer                           391\n",
      "Gose                               337\n",
      "Dubbel                             331\n",
      "Winter Warmer                      321\n",
      "Pumpkin Ale                        307\n",
      "Doppelbock                         294\n",
      "Cream Ale                          286\n",
      "Maibock / Helles Bock              280\n",
      "Schwarzbier                        277\n",
      "Irish Dry Stout                    270\n",
      "Quadrupel (Quad)                   265\n",
      "Irish Red Ale                      249\n",
      "Munich Dunkel Lager                238\n",
      "Bock                               233\n",
      "Altbier                            229\n",
      "Dunkelweizen                       219\n",
      "Smoked Beer                        219\n",
      "Kellerbier / Zwickelbier           218\n",
      "Light Lager                        210\n",
      "Belgian Dark Ale                   205\n",
      "Old Ale                            195\n",
      "Lambic - Fruit                     176\n",
      "Euro Dark Lager                    169\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "labels = df.groupby(['style']).size()\n",
    "print(labels.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of beers in IPA style: 0.133757282175\n"
     ]
    }
   ],
   "source": [
    "print('percent of beers in IPA style:', labels.IPA/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of styles used: 54\n",
      "\n",
      "Index(['Altbier', 'American Amber / Red Ale', 'American Black Ale',\n",
      "       'American Blonde Ale', 'American Double / Imperial IPA',\n",
      "       'American Lager', 'American Strong Ale', 'American Wild Ale',\n",
      "       'Barleywine', 'Belgian Dark Ale', 'Belgian Strong Dark Ale',\n",
      "       'Belgian Strong Pale Ale', 'Berliner Weissbier', 'Bitter', 'Bock',\n",
      "       'Brown Ale', 'Cream Ale', 'Czech Pilsener', 'Doppelbock', 'Dubbel',\n",
      "       'Dunkelweizen', 'Euro Dark Lager', 'Euro Pale Lager', 'Farm Ale',\n",
      "       'Fruit / Vegetable Beer', 'Gose', 'Hefeweizen', 'Herbed / Spiced Beer',\n",
      "       'IPA', 'Imperial Stout', 'Irish Dry Stout', 'Irish Red Ale',\n",
      "       'Kellerbier / Zwickelbier', 'Kölsch', 'Lager', 'Lambic - Fruit',\n",
      "       'Light Lager', 'Maibock / Helles Bock', 'Munich Dunkel Lager',\n",
      "       'Märzen / Oktoberfest', 'Old Ale', 'Pale Ale', 'Porter', 'Pumpkin Ale',\n",
      "       'Quadrupel (Quad)', 'Russian Imperial Stout', 'Rye Beer', 'Schwarzbier',\n",
      "       'Scottish Ale', 'Smoked Beer', 'Stout', 'Tripel', 'Wheat',\n",
      "       'Winter Warmer'],\n",
      "      dtype='object', name='style')\n"
     ]
    }
   ],
   "source": [
    "styles = df.groupby(['style']).size() \n",
    "print('Number of styles used:', len(styles))\n",
    "print('')\n",
    "print(styles.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "print('ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MACHINE LEARNING \n",
    "The most naive model would predict the most reviewed style: IPA.  It would be correct 13% of the time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (32802,)\n",
      "X_test shape: (32802,)\n"
     ]
    }
   ],
   "source": [
    "# PREDICT STYLE FROM REVIEWS\n",
    "# split into train and test data\n",
    "X = df['clean_review'].values\n",
    "y = df['style'].values\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3, random_state=22)\n",
    "print('X_train shape:',X_train.shape)\n",
    "print('X_test shape:',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32802, 596111)\n",
      "(14059, 596111)\n"
     ]
    }
   ],
   "source": [
    "# vectorize the train data, fit and transform into feature vectors\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=2, ngram_range = (1,2))\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "tfidf = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tfidf = tfidf.transform(X_train_counts)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16167579486449962"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NAIVE BAYES PREDICTOR\n",
    "%%time\n",
    "clf = MultinomialNB()\n",
    "# first pass, .1195,  after word cleaning .2076,  after combining styles .2639\n",
    "# ngrams (1,1): .2639  ngrams (1,2): .2117  ngrams (1,3): .20677\n",
    "# after adding stemmer with ngrams(1,2): .1617\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing\n",
      "CPU times: user 72 µs, sys: 40 µs, total: 112 µs\n",
      "Wall time: 118 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST  is very slow, so I'll start with a small portion:\n",
    "%%time\n",
    "X_train2 = X_train[:5000]\n",
    "y_train2 = y_train[:5000]\n",
    "X_test2 = X_test[10000:11000]\n",
    "y_test2 = y_test[10000:11000]\n",
    "\n",
    "# vectorize the train data, fit and transform into feature vectors\n",
    "vectorizer2 = CountVectorizer(analyzer='word', min_df=2, ngram_range = (1,2))\n",
    "X_train_counts2 = vectorizer2.fit_transform(X_train2)\n",
    "tfidf2 = TfidfTransformer().fit(X_train_counts2)\n",
    "X_train_tfidf2 = tfidf2.transform(X_train_counts2)\n",
    "X_test_counts2 = vectorizer2.transform(X_test2)\n",
    "X_test_tfidf2 = tfidf2.transform(X_test_counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40400000000000003"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RANDOM FOREST \n",
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(n_estimators = 300)\n",
    "forest_clf.fit(X_train_tfidf2, y_train2)\n",
    "predicted = forest_clf.predict(X_test_tfidf2)\n",
    "np.mean(predicted == y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46200000000000002"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RANDOM FOREST \n",
    "#first pass 0.45, after cleaning 0.426, after combined styles 0.462\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "text_clf= Pipeline([('vect', CountVectorizer(min_df=5)),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('forest',RFC(n_estimators=300)) ])\n",
    "text_clf = text_clf.fit(X_train2, y_train2)\n",
    "predicted = text_clf.predict(X_test2)\n",
    "np.mean(predicted == y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5983355857457856"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "#after cleaning, 0.575,  after combined styles 0.6218\n",
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "np.mean(predicted == y_test)\n",
    "print(np.mean)\n",
    "# best so far: .6218  increasing ngram lowers to .6060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "#this is quite slow.  don't run it until I can improve speed (min_df=5 is a start)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1,10],'tol':[0.001,0.0001,0.00001]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross-val score: {:.2f}'.format(grid.gest_score_))\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KMEANS COULD BE USED TO GROUP STYLES TOGETHER.  (use this before consolidating)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# DATA PREP\n",
    "group_df = df_copy\n",
    "# drop all reviews with < 20 characters\n",
    "group_df = group_df[group_df['review'].map(len) > 20]\n",
    "\n",
    "# Change review to a string of words.  remove non-letters, make lower case, split into words.  \n",
    "# Remove stopwords (common words.)  Join back together into a long string of words.  \n",
    "def review_to_words(review):\n",
    "    letters_only = re.sub('[^a-zA-Z]',' ', review)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words('english'))  \n",
    "    good_words = [w for w in words if not w in stops]\n",
    "    return(' '.join(good_words))\n",
    "\n",
    "# clean the reviews\n",
    "df['clean_group_df'] = df['review'].apply(review_to_words)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=7)\n",
    "train_data_features = vectorizer.fit_transform(df.clean_group_df)\n",
    "\n",
    "# Initialize the model with 2 parameters -- number of clusters and random state.\n",
    "kmeans_model = KMeans(n_clusters=20, random_state=22)\n",
    "# Get only the styles\n",
    "styles = group_df.style\n",
    "variables = train_data_features\n",
    "# Fit the model using the good columns.\n",
    "kmeans_model.fit(variables)\n",
    "# Get the cluster assignments.\n",
    "labels = kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 18.  18.   7. ...,   0.  15.   3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-47c4b509effb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpca_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Fit the PCA model on the numeric columns from earlier.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplot_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Make a scatter plot of each game, shaded according to cluster assignment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[0;32m--> 370\u001b[0;31m                         copy=self.copy)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# Handle n_components==None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 18.  18.   7. ...,   0.  15.   3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Import the PCA model.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA model.\n",
    "pca_2 = PCA(2)\n",
    "# Fit the PCA model on the numeric columns from earlier.\n",
    "plot_columns = pca_2.fit_transform(labels)\n",
    "# Make a scatter plot of each game, shaded according to cluster assignment.\n",
    "plt.scatter(x=plot_columns[:,0], y=plot_columns[:,1], c=labels)\n",
    "# Show the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# or maybe try k nearest neighbors\n",
    "\n",
    "from aklean.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 20)\n",
    "knn.fit()\n",
    "prediction = knn.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46861, 8)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_reviews = len(X_test)\n",
    "clean_test_reviews = []\n",
    "for b in range(0, num_reviews):\n",
    "    clean_test_reviews.append(review_to_words(X_test[b]))\n",
    "    \n",
    "test_data_features = vectorizer.transform(X_test)\n",
    "test_data_features = test_data_features.toarray()\n",
    "result = forest.predict(test_data_features)\n",
    "output = pd.DataFrame(data = {'id':test['id'], 'style':result})\n",
    "output_to_csv('BagOfWords_model.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5426\n",
      "{'nice': 220, 'auburn': 31, 'impressions': 163, 'tons': 350, 'of': 232, 'clarity': 72, 'solid': 308, 'inch': 165, 'off': 233, 'white': 372, 'head': 144, 'aroma': 28, 'was': 364, 'little': 192, 'bit': 45, 'sweet': 327, 'and': 21, 'nutty': 229, 'taste': 331, 'gave': 125, 'more': 213, 'sweetness': 328, 'stayed': 316, 'away': 33, 'from': 121, 'hops': 153, 'bitterness': 47, 'relatively': 278, 'light': 188, 'bodied': 50, 'nothing': 226, 'almond': 11, 'came': 61, 'out': 240, 'it': 171, 'that': 335, 'obvious': 230, 'kind': 177, 'fancied': 112, 'up': 358, 'oktoberfest': 234, 'while': 371, 'good': 132, \n"
     ]
    }
   ],
   "source": [
    "print(len(str(vectorizer.vocabulary_)))\n",
    "print(str(vectorizer.vocabulary_)[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 0 1 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 1 5 0 0 0 0 0 1 1 0 2 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 2 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 2 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      "  0 0 0 2 0 0 0 2 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0 0 2 7 0 0 0 0 0 6 0 0 0 0 2 0 0 0 0 0 1 0 0 0 1 0 0 0 0 4 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X2 = [df['review'][0]] \n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X)\n",
    "vector = vectorizer.transform(X2)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5426\n",
      "{'nice': 220, 'auburn': 31, 'impressions': 163, 'tons': 350, 'of': 232, 'clarity': 72, 'solid': 308, 'inch': 165, 'off': 233, 'white': 372, 'head': 144, 'aroma': 28, 'was': 364, 'little': 192, 'bit': 45, 'sweet': 327, 'and': 21, 'nutty': 229, 'taste': 331, 'gave': 125, 'more': 213, 'sweetness': 328, 'stayed': 316, 'away': 33, 'from': 121, 'hops': 153, 'bitterness': 47, 'relatively': 278, 'light': 188, 'bodied': 50, 'nothing': 226, 'almond': 11, 'came': 61, 'out': 240, 'it': 171, 'that': 335, 'obvious': 230, 'kind': 177, 'fancied': 112, 'up': 358, 'oktoberfest': 234, 'while': 371, 'good': 132, \n"
     ]
    }
   ],
   "source": [
    "print(len(str(vectorizer.vocabulary_)))\n",
    "print(str(vectorizer.vocabulary_)[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 1 5 0 0 0 0 0 1 1 0 2 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 2 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 2 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      "  0 0 0 2 0 0 0 2 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0 0 2 7 0 0 0 0 0 6 0 0 0 0 2 0 0 0 0 0 1 0 0 0 1 0 0 0 0 4 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform(X2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5426\n",
      "{'nice': 220, 'auburn': 31, 'impressions': 163, 'tons': 350, 'of': 232, 'clarity': 72, 'solid': 308, 'inch': 165, 'off': 233, 'white': 372, 'head': 144, 'aroma': 28, 'was': 364, 'little': 192, 'bit': 45, 'sweet': 327, 'and': 21, 'nutty': 229, 'taste': 331, 'gave': 125, 'more': 213, 'sweetness': 328, 'stayed': 316, 'away': 33, 'from': 121, 'hops': 153, 'bitterness': 47, 'relatively': 278, 'light': 188, 'bodied': 50, 'nothing': 226, 'almond': 11, 'came': 61, 'out': 240, 'it': 171, 'that': 335, 'obvious': 230, 'kind': 177, 'fancied': 112, 'up': 358, 'oktoberfest': 234, 'while': 371, 'good': 132, \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(X)\n",
    "# summarize\n",
    "print(len(str(vectorizer.vocabulary_)))\n",
    "print(str(vectorizer.vocabulary_)[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.20350706  7.90825515  7.90825515 ...,  7.50279005  7.90825515\n",
      "  7.90825515]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384)\n",
      "[[ 0.01357063  0.01357063  0.0407119   0.01357063  0.02714126  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.02714126  0.05428253\n",
      "   0.13570632  0.01357063  0.01357063  0.01357063  0.08142379  0.01357063\n",
      "   0.01357063  0.01357063  0.02714126  0.36640707  0.01357063  0.01357063\n",
      "   0.02714126  0.01357063  0.01357063  0.0407119   0.01357063  0.01357063\n",
      "   0.14927696  0.01357063  0.01357063  0.01357063  0.02714126  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.05428253  0.13570632  0.01357063\n",
      "   0.01357063  0.01357063  0.02714126  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.02714126  0.0407119   0.01357063  0.01357063\n",
      "   0.01357063  0.0407119   0.02714126  0.0407119   0.01357063  0.10856506\n",
      "   0.02714126  0.02714126  0.02714126  0.01357063  0.01357063  0.01357063\n",
      "   0.0407119   0.02714126  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.02714126  0.06785316  0.0407119   0.01357063  0.0407119   0.02714126\n",
      "   0.0407119   0.01357063  0.01357063  0.05428253  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.01357063  0.02714126\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.02714126  0.02714126  0.01357063  0.02714126  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.02714126  0.09499443  0.01357063  0.01357063\n",
      "   0.01357063  0.02714126  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.0407119   0.01357063  0.02714126  0.01357063  0.01357063\n",
      "   0.02714126  0.01357063  0.02714126  0.01357063  0.01357063  0.08142379\n",
      "   0.01357063  0.01357063  0.01357063  0.05428253  0.08142379  0.01357063\n",
      "   0.08142379  0.01357063  0.01357063  0.01357063  0.02714126  0.01357063\n",
      "   0.01357063  0.01357063  0.02714126  0.0407119   0.01357063  0.05428253\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.02714126  0.01357063  0.17641822  0.01357063  0.01357063  0.01357063\n",
      "   0.05428253  0.01357063  0.14927696  0.18998885  0.02714126  0.01357063\n",
      "   0.01357063  0.06785316  0.01357063  0.01357063  0.01357063  0.02714126\n",
      "   0.08142379  0.02714126  0.01357063  0.01357063  0.01357063  0.02714126\n",
      "   0.01357063  0.01357063  0.09499443  0.01357063  0.05428253  0.01357063\n",
      "   0.0407119   0.01357063  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.06785316  0.02714126  0.01357063  0.06785316  0.01357063\n",
      "   0.01357063  0.0407119   0.05428253  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.06785316  0.01357063  0.01357063\n",
      "   0.02714126  0.01357063  0.05428253  0.0407119   0.08142379  0.09499443\n",
      "   0.01357063  0.01357063  0.05428253  0.05428253  0.05428253  0.01357063\n",
      "   0.02714126  0.06785316  0.01357063  0.01357063  0.25784201  0.0407119\n",
      "   0.05428253  0.0407119   0.05428253  0.0407119   0.01357063  0.01357063\n",
      "   0.08142379  0.01357063  0.01357063  0.01357063  0.01357063  0.02714126\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.01357063  0.02714126\n",
      "   0.01357063  0.0407119   0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.0407119   0.01357063  0.0407119   0.01357063  0.01357063  0.02714126\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.02714126  0.01357063  0.01357063  0.06785316  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.02714126  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.01357063  0.05428253  0.01357063\n",
      "   0.05428253  0.01357063  0.01357063  0.01357063  0.01357063  0.02714126\n",
      "   0.01357063  0.02714126  0.01357063  0.0407119   0.01357063  0.02714126\n",
      "   0.01357063  0.01357063  0.01357063  0.02714126  0.01357063  0.01357063\n",
      "   0.02714126  0.02714126  0.01357063  0.02714126  0.01357063  0.01357063\n",
      "   0.01357063  0.0407119   0.01357063  0.02714126  0.0407119   0.0407119\n",
      "   0.01357063  0.08142379  0.01357063  0.01357063  0.01357063  0.08142379\n",
      "   0.36640707  0.01357063  0.01357063  0.02714126  0.02714126  0.01357063\n",
      "   0.27141265  0.01357063  0.01357063  0.01357063  0.01357063  0.20355949\n",
      "   0.05428253  0.01357063  0.01357063  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.02714126  0.08142379  0.01357063\n",
      "   0.01357063  0.02714126  0.10856506  0.01357063  0.16284759  0.01357063\n",
      "   0.02714126  0.02714126  0.01357063  0.01357063  0.01357063  0.02714126\n",
      "   0.0407119   0.01357063  0.21713012  0.01357063  0.01357063  0.01357063\n",
      "   0.01357063  0.01357063  0.01357063  0.02714126  0.01357063  0.02714126]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform([X[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'smell': 11165, 'soft': 11263, 'hop': 6090, 'aroma': 918, 'with': 13621, 'significant': 10951, 'malt': 7438, 'scents': 10579, 'this': 12373, 'one': 8499, 'smells': 11171, 'very': 13165, 'creamy': 3179, 'taste': 12183, 'and': 760, 'it': 6575, 'is': 6562, 'the': 12317, 'traditional': 12594, 'irish': 6546, 'flavors': 4964, 'come': 2790, 'out': 8606, 'at': 1005, 'tongue': 12522, 'not': 8299, 'like': 7146, 'cream': 3171, 'ale': 629, 'but': 2030, 'close': 2653, 'big': 1452, 'buttery': 2039, 'smooth': 11194, 'hops': 6116, 'are': 896, 'unique': 12939, 'sharp': 10812, 'flavor': 4957, 'an': 750, 'easy': 4133, 'saturated': 10524, 'well': 13435, 'mixed': 7865, 'blend': 1560, 'that': 12312, 'plays': 9177, 'complimenting': 2866, 'second': 10666, 'fiddle': 4818, 'to': 12482, 'base': 1222, 'no': 8248, 'sweetness': 12034, 'finish': 4878, 'nutty': 8357, 'changes': 2388, 'personalities': 9001, 'end': 4289, 'mouthfeel': 8004, 'lightly': 7135, 'carbonated': 2203, 'exceptionally': 4515, 'drinkability': 400\n",
      "236226\n"
     ]
    }
   ],
   "source": [
    "# vectorize with a list of reviews\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X = df['review'][0:2000]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(X)\n",
    "# summarize\n",
    "print(str(vectorizer.vocabulary_)[:1000])  # print a slice because this is very long\n",
    "print(len(str(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n",
      "[[ 0.07887912  0.01314652 -0.0657326   0.02629304 -0.0657326  -0.17090475\n",
      "  -0.01314652  0.         -0.28922343  0.01314652  0.10517215  0.01314652\n",
      "   0.28922343 -0.14461171 -0.01314652  0.01314652 -0.02629304 -0.10517215\n",
      "   0.21034431  0.          0.07887912  0.14461171 -0.07887912  0.02629304\n",
      "  -0.21034431  0.07887912 -0.14461171  0.14461171  0.01314652  0.\n",
      "   0.02629304 -0.03943956  0.24978387  0.14461171  0.07887912  0.15775823\n",
      "   0.07887912 -0.02629304  0.23663735  0.13146519 -0.22349083  0.19719779\n",
      "   0.         -0.03943956  0.07887912 -0.36810254  0.36810254 -0.07887912\n",
      "  -0.03943956  0.01314652]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "X = [df['review'][2]]  # start with a long review\n",
    "X2 = [df['review'][0]] # X2 is shorter\n",
    "\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=50)\n",
    "# encode document\n",
    "vector = vectorizer.transform(X)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = df.groupby(['style']).size()\n",
    "styles = counts.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT RATING FROM REVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 32802 y_train: 32802\n",
      "X_test: 14059 y_test: 14059\n",
      "X_train shape: (32802,)\n",
      "y_train shape: (32802,)\n"
     ]
    }
   ],
   "source": [
    "# PREDICT RATING FROM REVIEWS\n",
    "# split into train and test data\n",
    "X = df['clean_review'].values\n",
    "y = df['rating'].values\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3, random_state=22)\n",
    "print('X_train:',len(X_train), 'y_train:',len(y_train))\n",
    "print('X_test:', len(X_test), 'y_test:', len(y_test))\n",
    "print('X_train shape:',X_train.shape)\n",
    "print('y_train shape:',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce size of sets for faster algorithm testing\n",
    "X_train = X_train[:2000]\n",
    "y_train = y_train[:2000]\n",
    "X_test = X_test[5000:6000]\n",
    "y_test = y_test[5000:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf= Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('regr', linear_model.LinearRegression()) ])\n",
    "\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "difference = y_test-predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = y_test-predicted    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.22904293e-02   4.30080270e-01   3.07129316e-01   3.33224546e-01\n",
      "  -3.62324824e-01   2.26792364e-01   1.82471726e-01   2.71587523e-01\n",
      "  -9.14003607e-01  -2.83783566e-01   1.12203135e-01  -7.86179035e-01\n",
      "   2.55413171e-02   2.47920307e-01  -3.47294998e-01  -9.60459486e-03\n",
      "  -1.85688737e-01   3.32856648e-01   2.43453957e-01  -3.34692679e-01\n",
      "   6.20290975e-01  -1.70918108e-01  -6.23704007e-01  -3.05935000e-01\n",
      "  -1.01381520e-01   3.42858498e-01  -4.98501533e-02  -6.81189862e-01\n",
      "   4.22837160e-01   6.59763853e-01   7.04893342e-02   6.88984576e-01\n",
      "   5.25820207e-01   1.43343739e-01   2.85839310e-01   4.63002920e-02\n",
      "  -4.96747496e-02  -1.06130122e-01  -4.63480618e-01   2.54956202e-01\n",
      "  -1.70072952e-01  -2.05935000e-01  -1.61834974e-01  -8.00912618e-02\n",
      "  -5.33792091e-01  -5.87674572e-02   2.34913114e-01   1.57942280e-01\n",
      "  -9.73148841e-02   1.74652154e-01   5.53155557e-01   9.97980054e-01\n",
      "   7.12558923e-01   1.57332154e-01  -6.20166066e-01   9.16363116e-02\n",
      "  -4.05127929e-01   1.89720432e-01  -8.58514793e-02   9.97192903e-03\n",
      "   5.34193197e-01  -1.92475856e-01  -1.34259306e-01  -6.59564773e-02\n",
      "   2.19850007e-01  -6.72666360e-02  -2.12873654e-01   1.84740002e-01\n",
      "   2.16107016e-01  -3.85588525e-01  -1.81445573e-01  -3.53751459e-01\n",
      "   2.24473238e-03  -4.25323957e-01   1.37301526e-01  -7.27847021e-02\n",
      "   6.64679406e-01  -1.49350831e-01  -2.55677797e-01   6.06553303e-01\n",
      "  -7.47384318e-01   1.71171977e-01  -2.06834858e-02   3.74575903e-01\n",
      "  -3.90502167e-01  -3.61585125e-01  -4.70031616e-01   2.98813562e-01\n",
      "   5.44562139e-01   2.66680530e-01  -6.55612512e-02  -2.40942510e-01\n",
      "  -1.67981193e-01   8.59335826e-01   5.49240123e-02   1.68591684e-01\n",
      "   1.22119346e-02  -1.98961487e-01   3.99909605e-01  -2.06808355e-01\n",
      "   1.00480586e-01  -3.50615975e-01  -1.80074364e-02   6.22042537e-01\n",
      "  -9.87825284e-02   3.12335333e-01  -1.99786177e-01  -8.35647381e-01\n",
      "   8.57458009e-02  -4.07421634e-01   3.84753889e-01   2.10985104e-01\n",
      "  -4.44221959e-01   1.16997530e+00  -2.69675988e-01   9.95009424e-02\n",
      "   4.65965264e-01  -2.58806248e-01  -1.07732001e-08   1.33476787e-01\n",
      "  -4.14018267e-01  -7.14750487e-02   1.06802767e-02   8.11660007e-02\n",
      "   2.37214531e-01   1.37379741e-01  -6.93609808e-02   1.95290359e-01\n",
      "   2.88926735e-02   4.61276693e-02   4.66123740e-01   1.02879444e-02\n",
      "  -6.01552448e-01  -4.33463645e-01  -1.14659464e-01   3.26799403e-01\n",
      "   2.46700154e-01   2.19239130e-01  -4.16743812e-01   1.32408449e-01\n",
      "  -4.06682312e-02   1.70844000e-02   1.97094567e-01  -3.68155017e-01\n",
      "   9.92734147e-02  -1.99853278e-02  -2.45702702e-01  -1.25385055e+00\n",
      "   7.60972274e-02   1.15193387e-02   3.57272597e-01   1.10082794e-01\n",
      "  -1.28453272e-02   1.74177793e-01  -3.95319787e-01   8.15984803e-02\n",
      "  -1.68822779e-01   2.13886052e-01   7.20280384e-02  -2.70396442e-01\n",
      "   3.23600338e-01  -3.04155513e-01  -2.90464948e-01   5.95863579e-01\n",
      "  -1.19133825e-02  -1.83976536e-01  -3.82387202e-01   5.86007084e-02\n",
      "   1.94916216e-02   7.30885611e-02   3.36307522e-01  -2.17247044e-01\n",
      "   5.95478614e-01   8.30754474e-02  -1.93539648e-01  -3.27959249e-01\n",
      "   5.07315230e-01  -5.11381150e-01   4.39036279e-02  -6.34519565e-01\n",
      "   9.94216543e-02   1.44840478e-01   3.74066688e-01  -3.05796995e-01\n",
      "   8.77172747e-05   1.01126410e+00   1.68748868e-01  -5.94660390e-02\n",
      "   3.11476820e-01  -7.92373698e-01  -5.99477009e-01   1.97726378e-02\n",
      "   3.13727966e-02  -6.93895968e-01  -2.55451838e-01  -2.68387865e-01\n",
      "   3.56694121e-01   6.56280879e-01  -1.25653290e-01  -4.30350502e-02\n",
      "   2.27852483e-02   1.07188512e-01  -1.62783059e+00  -1.04405596e-01\n",
      "  -4.19408365e-01   7.34067375e-02   4.08932083e-02  -5.69944641e-02\n",
      "  -1.53085223e-01   5.89108524e-01   2.67922484e-01   1.44168427e-01\n",
      "  -1.06466632e-01  -1.04523304e-01  -5.28807715e-02  -5.92307539e-01\n",
      "   4.45622390e-01  -3.53309302e-01   2.06391307e-01  -2.71569975e-01\n",
      "  -1.06392469e-01   4.82250197e-01   2.42838366e-01   3.70613982e-01\n",
      "  -1.50717257e-01   6.47051196e-01   6.53609262e-01  -3.12893309e-01\n",
      "   2.89620574e-01   2.85539688e-01  -4.32096924e-01   3.38882728e-01\n",
      "  -3.56413522e-02   1.47520385e-01  -1.70454400e-01   9.22158969e-02\n",
      "   2.17765342e-01  -2.21636167e-01  -1.64789267e-01  -3.67184042e-02\n",
      "   7.53325576e-01   2.80072613e-01  -1.25968305e-01   3.26460886e-01\n",
      "  -2.36599621e-01   1.43500584e-01   1.12492935e-01  -7.42685324e-03\n",
      "   3.14615771e-01   2.07860392e-01   2.86778809e-01   2.36133600e-02\n",
      "  -2.29001181e-01   1.85384318e-01  -2.61491812e-01   5.14065000e-01\n",
      "   4.46902039e-01  -2.06074200e-01  -4.97521044e-02   4.17872567e-01\n",
      "   1.48916494e-01  -1.80906496e-01   2.47652211e-01   3.18659125e-01\n",
      "   8.68216943e-01   7.76715658e-01  -4.05935000e-01  -1.45098151e-01\n",
      "   4.82841060e-01   5.00519518e-01   1.35850960e-01   1.13543482e-01\n",
      "  -8.04153393e-01   8.53403571e-02  -1.56532618e-01   4.31732231e-02\n",
      "  -6.11847408e-02   2.61250129e-01   1.48416521e-01  -5.13984584e-02\n",
      "  -2.73798394e-01  -1.29789276e-01  -7.27323399e-01   1.25604022e-01\n",
      "   1.33954274e+00   4.45089217e-01  -1.46582676e-03   2.17011238e-01\n",
      "   1.68899797e-01   5.59752333e-09  -1.34095551e-01   3.74954853e-02\n",
      "   2.33725418e-01   2.08839661e-01  -2.31973719e-01   1.08338593e-01\n",
      "  -1.30715149e-01   7.32407764e-02   2.43345604e-01  -1.95269522e-01\n",
      "   6.29360040e-02   4.73712447e-09   1.16464183e-01  -3.46420822e-01\n",
      "  -3.31577806e-01  -2.79399514e-01  -3.91644175e-02   5.55348321e-01\n",
      "  -4.85093485e-02  -8.04380379e-02   4.58705734e-02  -1.16141350e-01\n",
      "   6.42154416e-03  -2.78438149e-01   6.96503550e-02   4.55827864e-02\n",
      "   4.24065000e-01   1.25603296e-01  -1.84237447e-01  -1.58838666e-01\n",
      "   1.78967045e-01  -7.68204037e-02   1.75074853e-01  -5.47902912e-01\n",
      "  -4.99197332e-02  -1.19580252e-01   1.02067950e-08  -1.17310057e-01\n",
      "   5.71252454e-01   3.03365965e-01  -3.40190917e-01  -7.23255766e-01\n",
      "  -7.61728414e-01   5.59206116e-01   5.11882159e-01   4.52420226e-02\n",
      "   6.08113090e-01   5.21508479e-01  -5.60535703e-01  -9.53051147e-02\n",
      "   2.92448841e-01   2.46929023e-01  -9.39823013e-02  -1.13634707e-01\n",
      "   7.62413560e-02   2.12831063e-01   3.90506330e-01   1.19600148e-01\n",
      "   2.10442497e-01   2.00627128e-02  -1.26878951e-01   3.94635332e-01\n",
      "   3.40184244e-02  -2.12868113e-01  -1.68457965e-01  -3.11760268e-01\n",
      "  -4.83777917e-01   8.49721904e-02   1.19132792e-02   7.44552784e-02\n",
      "  -2.19056066e-01   1.32424341e-01  -8.76146303e-02   4.22043758e-01\n",
      "   2.58199467e-02   4.25193054e-01  -2.16546075e-01   2.74132291e-01\n",
      "  -2.16952308e-01  -4.26477970e-01   7.29189450e-02   1.21507892e-01\n",
      "  -2.63721353e-01  -1.51706283e-02   8.81306950e-02  -1.13464766e-01\n",
      "  -1.28753423e+00  -4.18978594e-01   8.30723947e-02   2.67536914e-01\n",
      "   3.18504465e-01  -1.91113061e-01   6.39685689e-02   1.04905258e-08\n",
      "  -7.83300683e-02  -2.82764423e-01  -6.50695624e-01  -7.03928067e-02\n",
      "  -2.71372496e-01  -1.00687031e+00   1.33272757e-01  -8.01253540e-02\n",
      "  -8.42546009e-01  -5.17274374e-02   8.74354296e-03  -2.00125865e-01\n",
      "  -3.43812679e-01   3.52259354e-01   6.65299395e-02  -4.10675313e-02\n",
      "  -1.76521736e-01   5.26708988e-01   2.76002183e-01  -3.63014688e-01\n",
      "   2.06897689e-01  -6.47712264e-01  -7.92910773e-02   4.31662502e-01\n",
      "   1.42461707e-01   3.17452836e-02  -1.06195882e-02  -6.04635378e-02\n",
      "  -1.64286274e-01   2.74065000e-01  -8.81025443e-02   4.19060414e-01\n",
      "  -3.41215352e-01   3.12899002e-01   1.26715136e-01  -5.13967818e-01\n",
      "   1.72563756e-01   1.66367297e-01  -1.76156134e-02  -7.10685282e-01\n",
      "   2.55019983e-02   2.04530489e-01   4.91505532e-01   1.54356419e-01\n",
      "   1.21468382e-01  -1.98807120e-01   1.43807337e-01   2.59488266e-01\n",
      "   4.98460414e-01  -1.32121249e+00   2.42584845e-01  -8.05155417e-03\n",
      "  -3.03234389e-01  -4.02955231e-02  -8.96518918e-01   3.99141895e-01\n",
      "   4.06687202e-01  -6.21793677e-02  -1.75224507e-01  -2.72546548e-01\n",
      "   2.40710204e-01   3.21867075e-01   2.33092314e-02  -2.40180553e-01\n",
      "  -3.94981334e-01  -3.67665780e-01   2.38524652e-01   4.69140905e-01\n",
      "   9.05410278e-02  -5.94832548e-01  -4.07489647e-01  -1.09836803e-01\n",
      "  -4.19211845e-01   1.25226376e-02  -1.55452777e-01   1.30025537e-01\n",
      "  -1.29552151e-02   2.80382030e-01   4.54654078e-01   2.63287398e-01\n",
      "  -1.84782541e-02   2.10394032e-01   1.12554589e-01   2.72994969e-01\n",
      "   3.01488217e-01  -2.16305603e-01   6.72961122e-02  -9.48812180e-02\n",
      "   3.20905506e-01   7.29705436e-02  -9.54128033e-02   2.81772669e-01\n",
      "   1.05887830e-01   1.59736705e-01   3.10536060e-01  -2.21913003e-01\n",
      "   1.21428949e-01   7.44065000e-01   3.65763975e-01   8.10232949e-02\n",
      "   7.67127652e-01   1.60873035e-01  -9.86162281e-02  -1.99348072e-01\n",
      "   2.23284116e-02  -6.40558173e-02  -3.26323125e-01  -1.03210335e-01\n",
      "   3.08022604e-01  -1.42715527e-01   2.97095221e-01  -1.98676048e-01\n",
      "   1.17163377e-01   3.66488106e-01  -1.37986955e-01   6.18249264e-01\n",
      "  -3.43046964e-03   2.24471980e-01  -3.15805905e-01  -6.20890352e-02\n",
      "  -7.54756474e-01   1.47121997e-01  -3.13636361e-01   3.65905868e-01\n",
      "   2.18037225e-01   2.66286893e-01   2.84012167e-01   1.45358137e-01\n",
      "  -3.11823979e-01   1.66149512e-01  -5.55274586e-01   6.04370404e-02\n",
      "   6.87894646e-02  -1.80414697e-01   1.18155765e-01   4.40049071e-01\n",
      "   1.78380943e-01  -1.80825247e-02   4.55635070e-01  -1.77552758e-01\n",
      "   4.78402606e-02   9.82785985e-02  -3.49978129e-01  -1.22515374e-01\n",
      "  -3.57449621e-01   2.95923400e-01  -9.73454627e-02   1.39660663e-01\n",
      "  -2.65687269e-02   2.58967998e-01   6.03407642e-01  -6.32302299e-09\n",
      "  -6.29857915e-01   6.12399590e-02  -2.27240651e-01   9.10354369e-01\n",
      "   9.00778619e-02   5.48585035e-01  -1.10097173e-01  -9.36456579e-02\n",
      "  -1.28371743e-01  -1.80754932e-01   1.80034955e-01   5.22162460e-01\n",
      "   3.62757829e-01  -5.98684277e-02  -1.94599145e-01   1.62311616e-01\n",
      "  -6.82151916e-01  -6.95781691e-01   1.83684926e-01   2.78941232e-01\n",
      "  -2.99925228e-01   7.19476977e-01   3.06850107e-01  -2.05935000e-01\n",
      "   2.36960686e-02   6.22667359e-01   3.52610690e-01  -2.09874906e-01\n",
      "  -2.92555284e-01  -3.89248782e-02   9.40491349e-02   1.05605226e+00\n",
      "  -8.92335280e-02  -2.12503893e-02   7.36472335e-02   1.32950247e-01\n",
      "   1.31423982e-01   3.54227159e-01   4.75126937e-01   3.83832272e-01\n",
      "   2.30269325e-01   3.83953874e-01  -2.38128674e-01   1.49810730e-01\n",
      "  -3.66254293e-01  -3.36001923e-01  -6.37659504e-02   1.54709360e-01\n",
      "   3.37300454e-02  -1.33788736e-02  -2.44748637e-01   1.91416860e-01\n",
      "  -2.54067180e-01  -3.23250097e-01   6.58520127e-09   1.07647938e-01\n",
      "  -2.99334370e-01   2.72338326e-01   3.07795015e-01  -4.25631543e-02\n",
      "  -3.53066411e-01   4.16296004e-01   9.78165762e-02   4.47500567e-01\n",
      "   3.65845851e-01   3.72276109e-01   1.85460188e-01   5.69948260e-01\n",
      "   4.15728919e-02   1.69897075e-02   2.59129418e-01  -4.21006759e-02\n",
      "   1.62189476e-01   3.35226500e-01  -2.38162937e-01  -6.81165053e-02\n",
      "   6.91056361e-01  -4.23561207e-01   2.07677845e-01   1.93418355e-01\n",
      "   2.86297897e-02  -1.34519550e+00  -1.25869376e+00  -5.09669237e-01\n",
      "   2.45993179e-01   3.12088588e-01  -4.42715491e-01   3.15701901e-02\n",
      "  -3.28265267e-01  -1.61258300e+00   3.30941382e-02   3.66062740e-01\n",
      "  -8.50407142e-01  -1.95938410e-01  -1.30765148e-02  -1.93576846e-01\n",
      "   1.62718547e-01  -3.29924507e-02   2.94444512e-03   7.06968904e-02\n",
      "   1.65854437e-01   1.27270381e-01   4.22776228e-01  -4.49568718e-02\n",
      "  -2.55604647e-01   4.12015182e-01  -2.11422643e-01  -2.00281813e-01\n",
      "   6.40557777e-02   1.74178652e-01  -1.60365175e-01   5.80494564e-02\n",
      "  -3.62687129e-01  -2.83885020e-01   6.01851321e-01   3.65718068e-01\n",
      "  -2.46707874e-01  -1.06575435e-01   5.59853530e-01   4.57618493e-02\n",
      "   2.41348811e-01  -1.28224785e-02   1.89272317e-01  -3.03494034e-01\n",
      "   5.66213030e-01   4.64210162e-01   2.40564352e-01  -4.66394500e-01\n",
      "   2.32652831e-01  -1.54112766e-01  -3.12925485e-01   3.55979347e-01\n",
      "   1.52513451e-01  -3.64247154e-02  -3.68028580e-01   6.24786535e-01\n",
      "  -2.18852497e-01  -1.05714924e-01  -4.12740261e-02  -3.86045671e-01\n",
      "   2.79699621e-02   4.39006302e-01   1.71913149e-01  -6.32389029e-02\n",
      "  -2.26429384e-02  -4.20685840e-01   1.55914193e-01   2.07531846e-01\n",
      "   1.49025151e-01  -1.09371758e+00   3.39612347e-01  -3.80585569e-01\n",
      "  -1.68192203e-01   2.01939046e-01  -1.75906387e-02  -2.40098387e-01\n",
      "  -1.67225991e-01   1.75473265e-01   1.02591365e-01  -4.40160787e-01\n",
      "   8.83536222e-09  -9.30137629e-01   3.63612831e-01  -2.12949776e-01\n",
      "   1.76410327e-01  -2.72075924e-01   4.40048958e-01   3.18884365e-01\n",
      "  -4.86355407e-01   1.23983870e-02   6.09298514e-03   2.99836108e-01\n",
      "   2.48950670e-02  -9.51794279e-01  -9.62084918e-02  -2.29951539e-01\n",
      "  -1.11703917e+00  -7.07366480e-02   4.92657808e-02  -4.99633010e-01\n",
      "   1.87341887e-02  -2.12209449e-01   2.28896970e-01   2.41195238e-01\n",
      "   3.67083045e-01  -1.18728007e-01  -1.45514648e-01   7.08394911e-02\n",
      "   3.44987071e-01  -1.32261597e-02  -4.74607991e-02  -4.38661736e-01\n",
      "  -4.61625747e-01   6.10276121e-01   6.54588348e-01   2.38171669e-01\n",
      "  -1.46926459e-01  -6.41321492e-02  -3.71271341e-01   1.53476001e-01\n",
      "  -7.49665064e-02  -8.34911556e-03   6.42174151e-02  -6.62287203e-01\n",
      "  -4.32740706e-02  -9.64246441e-02   3.69735990e-02  -4.21436454e-01\n",
      "   1.85357730e-01   7.71734188e-02  -4.31070665e-01  -1.63930790e-01\n",
      "  -7.95880983e-02  -3.82247574e-01  -1.47178238e-01   2.38370518e-01\n",
      "  -1.21399983e-01  -2.82952934e-01  -2.56113616e-02   3.65650622e-01\n",
      "   2.36830601e-02   1.83570343e-01  -1.21203132e-02   6.78891030e-01\n",
      "  -6.34378363e-01  -3.92105186e-01   9.77314796e-02   2.21660384e-01\n",
      "  -1.44574713e-01  -1.47921275e-02   2.90398875e-03  -3.30561831e-02\n",
      "  -2.00855045e-01  -2.26727140e-02   1.33941423e-01  -2.65768719e-01\n",
      "   4.85479380e-02   1.03382050e-01   2.69644761e-01  -7.49909229e-02\n",
      "   2.68233958e-01  -9.98019830e-01   4.03995651e-02   5.32691587e-01\n",
      "  -6.99619578e-02   1.73542395e-01  -5.25123994e-02  -4.69476506e-01\n",
      "  -6.05617135e-02  -1.97530703e-01  -1.62004869e-01   2.79484848e-01\n",
      "  -2.87131446e-01   8.99423777e-02  -1.33439395e-01   5.59109897e-02\n",
      "  -5.08555144e-02   5.22889239e-01   3.79144367e-01   1.30469121e-01\n",
      "   1.15439678e+00  -5.56770820e-01  -6.18346977e-02   1.30751077e-02\n",
      "   5.59019695e-03   2.86398245e-01   5.63490526e-01   3.02858149e-01\n",
      "   2.05174415e-01  -8.92836294e-01  -3.62729279e-01   8.11779112e-02\n",
      "   3.87689696e-01  -3.32126177e-01  -2.13063509e-01  -4.57317159e-02\n",
      "   1.37591560e-01   7.81154402e-02   3.67687298e-01   1.52923795e-01\n",
      "   3.00516190e-02  -3.52953772e-01  -9.71281650e-02  -5.39553450e-02\n",
      "  -1.54608181e-01   4.91690053e-01   2.41801492e-02  -2.61787412e-01\n",
      "  -2.05635734e-01   6.63574615e-01  -1.86311320e-01   9.37811596e-02\n",
      "   3.05759271e-01   4.63532740e-01  -1.72982541e-01   1.53093001e-01\n",
      "   2.30592446e-01  -1.70837083e-01  -2.14915411e-01   5.09415587e-01\n",
      "   3.24228263e-01  -2.55952173e-01  -1.40748473e+00   1.33596171e-01\n",
      "  -2.05153347e-01   1.42253502e-01  -1.76109538e-02  -9.84085502e-02\n",
      "  -3.75233153e-01  -3.83553210e-01  -1.11593394e-01  -9.79146728e-01\n",
      "  -9.52522149e-02   2.56733226e-01   4.91251654e-02   8.91401016e-02\n",
      "  -2.46829206e-01  -1.37109210e+00   1.65430410e-01   5.09152237e-01\n",
      "   1.19273053e-01  -1.83338353e-01  -3.14275177e-02   3.50903544e-01\n",
      "   1.65956557e-01  -5.17002526e-01   1.93218848e-01  -4.32042744e-01\n",
      "  -2.36275283e-01   1.15262809e-01   2.44094426e-01  -4.75834414e-01\n",
      "   3.59354383e-01  -9.77910655e-01  -2.27947363e-01   6.49320038e-02\n",
      "   1.01645611e-01  -1.28217481e-01  -2.97235208e-01  -6.20730856e-02\n",
      "   9.90898576e-02  -5.63024039e-02   1.27969292e-01  -2.45686396e-01\n",
      "   4.24065000e-01  -8.31191839e-02  -9.85953684e-01  -2.41487215e-01\n",
      "   5.87621401e-02  -4.66526358e-01   2.39072910e-01  -4.02023752e-01\n",
      "   2.41024049e-01   1.57289555e-01   3.49887352e-01   1.84037618e-01\n",
      "   1.11202576e-01   2.27847952e-01   5.71214832e-01  -9.62323351e-01\n",
      "   2.15761991e-01  -7.66773467e-02  -6.20757669e-01   9.54881403e-02\n",
      "   4.32421655e-01   1.37404011e-01  -1.53331131e-01   4.23941451e-01\n",
      "   8.06608408e-02  -2.15476651e-01   1.83178006e-01  -3.05762410e-01\n",
      "   2.44545376e-02  -9.31921044e-02   4.88164408e-01   8.28199417e-01\n",
      "   7.35070662e-02   9.01399888e-09   6.96145355e-01  -3.26288341e-01\n",
      "  -5.25967071e-02  -2.29157413e-01  -1.22286887e+00   1.82942668e-01\n",
      "   4.55240853e-01   1.14091630e-01   3.17658287e-02   1.19273730e-01\n",
      "   5.36339940e-01  -1.20759913e-08  -2.68054350e-01   1.33112570e-02\n",
      "   4.14254851e-01  -3.22496230e-01  -1.84945042e-01   4.82883610e-01\n",
      "   2.38628956e-01   9.73317682e-02  -1.30640186e-01  -3.33527044e-01\n",
      "   4.79349438e-01   1.41650771e-01  -4.73574308e-01   6.00183170e-02\n",
      "  -2.34930078e-01  -5.56850953e-01  -4.22418155e-01  -2.24726714e-01\n",
      "   7.87880892e-01  -3.30396444e-02   2.22276528e-03  -1.84128139e-01\n",
      "   1.45356758e-01   4.65736743e-01  -5.13230928e-01  -2.12761167e-01\n",
      "   6.24558192e-01   1.67653402e-01  -8.29613971e-01  -2.18075564e-01\n",
      "   2.93738507e-01   3.12777486e-01  -2.74399310e-01   2.13702915e-01\n",
      "   2.49180405e-01  -3.54455518e-02   4.66342389e-01  -1.13731894e-02\n",
      "  -1.53782180e-01  -7.00184580e-01  -4.05000014e-01  -1.79953593e-01\n",
      "   6.12871765e-01   7.91284172e-02   1.84382756e-01   4.17888287e-01\n",
      "  -2.19290551e-01   4.39006302e-01   8.06292075e-01   1.03990135e-01\n",
      "  -1.50665175e-01  -3.85306236e-01  -3.79709173e-02   9.51266223e-02\n",
      "   8.28628195e-01  -3.61226946e-01   5.87327723e-01   2.58812847e-01\n",
      "  -2.30065596e-01  -3.37735312e-02   5.86595491e-01   5.71594520e-01\n",
      "  -1.29319673e-01   1.69679109e-01   4.77033868e-01   4.74739267e-01\n",
      "   3.78769612e-01  -5.29696777e-01   1.00691481e-01   1.64872465e-02\n",
      "  -9.90564826e-02   9.94003089e-02  -3.47115287e-03   5.61795700e-01]\n"
     ]
    }
   ],
   "source": [
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKcAAAFeCAYAAABQJXwLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a19ea6b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.0082647726811\n",
      "std: 0.364689021219\n"
     ]
    }
   ],
   "source": [
    "# difference of model from user rating\n",
    "import matplotlib.pyplot as plt\n",
    "_= plt.figure(figsize=(20,5))\n",
    "_= plt.hist(difference, bins=35, edgecolor=\"k\")\n",
    "_= plt.xticks(size=15)\n",
    "_= plt.yticks(size=15)\n",
    "_= plt.xlabel('difference', size=17)\n",
    "_= plt.ylabel('count', size=17)\n",
    "_= plt.title('Predicted rating vs Actual rating', size=17)\n",
    "plt.show()\n",
    "mean_diff = np.mean(difference)\n",
    "std_diff = np.std(difference)\n",
    "print('mean:',mean_diff)\n",
    "print('std:',std_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    " \n",
    "# initialise the SVM classifier\n",
    "classifier = LinearSVC()\n",
    " \n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([ 3.78,  3.16,  4.27, ...,  4.1 ,  4.01,  3.81]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-3cf36a377729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                    ('clf',MultinomialNB()) ])\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# first pass, .1195,  after word cleaning .2076,  after combining styles .2639\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mlabelbin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelbin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelbin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mShape\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mproblems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([ 3.78,  3.16,  4.27, ...,  4.1 ,  4.01,  3.81]),)"
     ]
    }
   ],
   "source": [
    "# Naive Bayes predictor\n",
    "\n",
    "text_clf= Pipeline([('vect', CountVectorizer(min_df=7)),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf',MultinomialNB()) ])\n",
    "\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RANDOM FOREST \n",
    "\n",
    "X_train2 = X_train[:5000]\n",
    "y_train2 = y_train[:5000]\n",
    "X_test2 = X_test[10000:10500]\n",
    "y_test2 = y_test[10000:10500]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "text_clf= Pipeline([('vect', CountVectorizer(min_df=5)),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('forest',RFC(n_estimators=300)) ])\n",
    "text_clf = text_clf.fit(X_train2, y_train2)\n",
    "predicted = text_clf.predict(X_test2)\n",
    "np.mean(predicted == y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-a0079b451ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtest_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mstyle_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     cv_loss = np.mean(cross_val_score(classifier, X_train_word_features, \n\u001b[0m\u001b[1;32m     21\u001b[0m                                       train_target, cv=5, scoring='neg_log_loss'))\n\u001b[1;32m     22\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "# CALCULATE PREDICTION FOR ONE STYLE  -- doesn't work on my data\n",
    "# from kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments\n",
    "# ValueError: Found input variables with inconsistent numbers of samples: [32802, 3857]\n",
    "# when calling cross_val_score.  I reduced df length in \"train_target = y_train[y_train==...]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "word_vect = TfidfVectorizer()\n",
    "word_vect.fit(X_train)\n",
    "X_train_word_features = word_vect.transform(X_train)\n",
    "test_features = word_vect.transform(X_test)\n",
    "\n",
    "style_names = ['American IPA','American Porter'] \n",
    "# ['American Pale Ale (APA)','Saison / Farmhouse Ale','American Double / Imperial IPA']\n",
    "losses = []\n",
    "auc = []\n",
    "for style_name in style_names:\n",
    "    #call the labels one at a time so we can run the classifier on them\n",
    "    train_target = y_train[y_train==style_name]\n",
    "    test_target = y_test[y_test==style_name]\n",
    "    classifier = LogisticRegression(solver='sag', C=10)\n",
    "    cv_loss = np.mean(cross_val_score(classifier, X_train_word_features, \n",
    "                                      train_target, cv=5, scoring='neg_log_loss'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV Log_loss score for class {} is {}'.format(style_name, cv_loss))\n",
    "    cv_score = np.mean(cross_val_score(classifier, X_train_word_features, \n",
    "                                       train_target, cv=5, scoring='accuracy'))\n",
    "    print('CV Accuracy score for class {} is {}'.format(style_name, cv_score))\n",
    "    \n",
    "    classifier.fit(X_train, train_target)\n",
    "    y_pred = classifier.predict(test_features)\n",
    "    y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n",
    "    auc_score = metrics.roc_auc_score(test_target, y_pred_prob)\n",
    "    auc.append(auc_score)\n",
    "    print(\"CV ROC_AUC score {}\\n\".format(auc_score))\n",
    "    \n",
    "    print(confusion_matrix(test_target, y_pred))\n",
    "    print(classification_report(test_target, y_pred))\n",
    "\n",
    "print('Total average CV Log_loss score is {}'.format(np.mean(losses)))\n",
    "print('Total average CV ROC_AUC score is {}'.format(np.mean(auc)))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### start over here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prep the data\n",
    "df = df_copy\n",
    "df = df.drop(['name','brewery'], axis=1)\n",
    "# drop all reviews with < 15 characters\n",
    "df = df[df['review'].map(len) > 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49291\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_id = 0\n",
    "text = df.loc[t_id, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-a38df8ab5db2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize, keywords\n",
    "\n",
    "word_scores = keywords(text, words=5, scores=True, split=True, lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* shape of the tfidf vectors (14267, 42882)\n",
      "reduce the dimensionality of the data\n",
      "* shape of the pca components (14267, 100)\n",
      "confidence: 0.676314427245\n",
      "training a support vector machine on first half\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-70fcb7445f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training a support vector machine on first half'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "#  Linear Regression predictor\n",
    "# most of this module is from Ralph Bean, github\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils import sparsefuncs\n",
    "\n",
    "\n",
    "n_samples = len(X_train)\n",
    "\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=(1,1))\n",
    "data = vect.fit_transform(X_test)\n",
    "print('* shape of the tfidf vectors', data.shape)\n",
    "vectors = data\n",
    "\n",
    "print('reduce the dimensionality of the data')\n",
    "pca = TruncatedSVD(n_components=100)\n",
    "data = pca.fit_transform(data)\n",
    "\n",
    "print('* shape of the pca components', data.shape)\n",
    "exp = np.var(data, axis=0)\n",
    "full = sparsefuncs.mean_variance_axis(vectors,axis=0)[1].sum()\n",
    "explained_variance_ratios = exp/full\n",
    "confidence = sum(explained_variance_ratios)\n",
    "print('confidence:',confidence)\n",
    "    \n",
    "print('training a support vector machine on first half')    \n",
    "regr = LinearRegression()\n",
    "regr.fit(data[:n_samples/2], data[y_train])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
